memory-hotplug
如下介绍来源于kernel/documentation/memory-hotplug.txt

内容：
1.简介
	1.1、purpose of memory hotplug				    内存热插拔的目的
	1.2、Phases of memory hotplug				    内存热插拔的阶段
	1.3、Unit of Memory online/offline operation	内存单元上线/离线操作
2.Kernel Configuration                内核配置
3.sysfs files for memory hotplug      内存热插拔sysfs files
4.Physical memory hot-add phase       物理内存热添加阶段
4.1、Hardware(Firmware) Support        Hardware(Firmware)支持
4.2、Notify memory hot-add event by hand
5.Logical Memory hot-add phase         逻辑内存 热添加阶段
5.1、State of memory				   内存状态
5.2、How to online memory			  怎样使内存上线
6.Logical memory remove               逻辑内存删除
6.1、Memory offline and ZONE_MOVABLE
6.2、How to offline memory
7.Physical memory remove              物理内存删除
8.Memory hotplug event notifier       内存热插拔事件通知

.. note::
    (1) x86_64's has special implementation for memory hotplug. This text does not describe it.
	(2) This text assumes that sysfs is mounted at /sys.

Introduction 
============

purpose of memory hotplug
-------------------------
内存热插拔允许用户增加/减少一定量的内存
Memory Hotplug allows users to increase/decrease the amount of memory. 
总的来说，有两个目的：
Generally, there are two purposes.

调整内存容量，这是为了允许按照需求容量进行划分；
(A) For changing the amount of memory. This is to allow a feature like capacity on demand. 
物理上安装或卸载 DIMMs 或NUMA-nodes 
(B) For installing/removing DIMMs or NUMA-nodes physically. 
交换DIMMs/NUMA-nodes,减少功耗
This is to exchange DIMMs/NUMA-nodes, reduce power consumption, etc.

(A) 是高度虚拟化环境所必需的，(B) 是支持内存电源管理的硬件所必需的。
(A) is required by highly virtualized environments and (B) is required by 
hardware which supports memory power management.

Linux memory hotplug is designed for both purpose.

Phases of memory hotplug
------------------------
There are 2 phases in Memory Hotplug:
  1) Physical Memory Hotplug phase         物理内存热插拔阶段
  2) Logical Memory Hotplug phase.         逻辑内存热插拔阶段

  
The First phase is to communicate hardware/firmware and make/erase 
environment for hotplugged memory. Basically, this phase is necessary 
for the purpose (B), but this is good phase for communication between 
highly virtualized environments too.  
第一阶段是为热插拔内存通信硬件/固件和创建/擦除环境。 
基本上，这个阶段对于目的（B）来说是必要的，
但这也是高度虚拟化环境之间通信的好阶段。

When memory is hotplugged, the kernel recognizes new memory, makes new memory 
management tables, and makes sysfs files for new memory's operation.
当内存被热插入时，内核会识别新内存，创建新的内存管理表，并为新内存的操作创建 sysfs 文件。

If firmware supports notification of connection of new memory to OS, this phase 
is triggered automatically. ACPI can notify this event. If not, "probe" operation 
by system administration is used instead. (see :ref:`memory_hotplug_physical_mem`).
如果固件支持新内存连接到操作系统的通知，则此阶段会自动触发。 
ACPI 可以通知此事件。 如果不是，则改为使用系统管理的“探测”操作。

Logical Memory Hotplug phase is to change memory state into available/unavailable
for users. Amount of memory from user's view is changed by this phase. The kernel 
makes all memory in it as free pages when a memory range is available.
逻辑内存热插拔阶段是将内存状态更改为对用户可用/不可用。 
此阶段更改了用户视角中的内存量。 当内存范围可用时，内核会将其中的所有内存作为空闲页面。

In this document, this phase is described as online/offline.

Logical Memory Hotplug phase is triggered by write of sysfs file by system administrator. 
For the hot-add case, it must be executed after Physical Hotplug phase by hand. 
(However, if you writes udev's hotplug scripts for memory hotplug, these phases 
can be execute in seamless way.)
逻辑内存热插拔阶段由系统管理员写入 sysfs 文件触发。 对于热添加情况，
必须在物理热插拔阶段之后手动执行。 
（但是，如果您为内存热插拔编写 udev 的热插拔脚本，则可以无缝地执行这些阶段。）

Unit of Memory online/offline operation
---------------------------------------

Memory hotplug uses SPARSEMEM memory model which allows memory to be divided into chunks 
of the same size. These chunks are called "sections". The size of a memory section is architecture 
dependent. For example, power uses 16MiB, ia64 uses 1GiB.
内存热插拔使用稀疏内存模型，该模型允许将内存分成相同大小的块。 这些块称为“sections”。 
内存部分的大小取决于架构。 例如，power 使用 16MiB，ia64 使用 1GiB。

Memory sections are combined into chunks referred to as "memory blocks". The size of a memory block 
is architecture dependent and represents the logical unit upon which memory online/offline operations 
are to be performed. The default size of a memory block is the same as memory section size unless an 
architecture specifies otherwise. (see :ref:`memory_hotplug_sysfs_files`.)
Memory sections被组合成称为“内存块”的块。 内存块的大小取决于体系结构，并表示要在其上执行内存
在线/离线操作的逻辑单元。 除非架构另有规定，否则内存块的默认大小与内存段大小相同。

To determine the size (in bytes) of a memory block please read this file:
/sys/devices/system/memory/block_size_bytes          可以看section大小

NT 平台大小为64MB，1<<26
59A 平台大小为1GB，1<<30

Kernel Configuration 
====================

To use memory hotplug feature, kernel must be compiled with following 
config options.

- For all memory hotplug:
    - Memory model -> Sparse Memory  (CONFIG_SPARSEMEM)
    - Allow for memory hot-add       (CONFIG_MEMORY_HOTPLUG)

- To enable memory removal, the following are also necessary:
    - Allow for memory hot remove    (CONFIG_MEMORY_HOTREMOVE)
    - Page Migration                 (CONFIG_MIGRATION)

- For ACPI memory hotplug, the following are also necessary:
    - Memory hotplug (under ACPI Support menu) (CONFIG_ACPI_HOTPLUG_MEMORY)
    - This option can be kernel module. 

- As a related configuration, if your box has a feature of NUMA-node hotplug 
via ACPI, then this option is necessary too.

    - ACPI0004,PNP0A05 and PNP0A06 Container Driver (under ACPI Support menu) 
	(CONFIG_ACPI_CONTAINER).
    
	This option can be kernel module too.

.. _memory_hotplug_sysfs_files:

sysfs files for memory hotplug 
==============================

All memory blocks have their device information in sysfs.  Each memory block 
is described under /sys/devices/system/memory as:
	/sys/devices/system/memory/memoryXXX
	(XXX is the memory block id.) 

For the memory block covered by the sysfs directory.  It is expected that all 
memory sections in this range are present and no memory holes exist in the range. 
Currently there is no way to determine if there is a memory hole, but the existence 
of one should not affect the hotplug capabilities of the memory block.

对于 sysfs 目录所覆盖的内存块。 预计该范围内的所有内存段都存在，并且该范围内不存在内存空洞。 
目前还没有办法确定是否存在内存漏洞，但存在不应该影响内存块的热插拔能力。

For example, assume 1GiB memory block size. A device for a memory starting at 
0x100000000 is /sys/device/system/memory/memory4::
	(0x100000000 / 1Gib = 4)
	
This device covers address range [0x100000000 ... 0x140000000)

Under each memory block, you can see 5 files:
- /sys/devices/system/memory/memoryXXX/phys_index
- /sys/devices/system/memory/memoryXXX/phys_device
- /sys/devices/system/memory/memoryXXX/state
- /sys/devices/system/memory/memoryXXX/removable
- /sys/devices/system/memory/memoryXXX/valid_zones

=================== ============================================================
``phys_index``      read-only and contains memory block id, same as XXX.
``state``           read-write

                    - at read:  contains online/offline state of memory.
                    - at write: user can specify "online_kernel",
					
                    "online_movable", "online", "offline" command
                    which will be performed on all sections in the block.
``phys_device``     read-only: designed to show the name of physical memory 
					device.  This is not well implemented now.
``removable``       read-only: contains an integer value indicating 
					whether the memory block is removable or not
                    removable.  A value of 1 indicates that the memory 
					block is removable and a value of 0 indicates that
                    it is not removable. A memory block is removable 
					only if every section in the block is removable.

``valid_zones``   	read-only: designed to show which zones this memory block 
					can be onlined to.
			The first column shows it`s default zone.
			
		    "memory6/valid_zones: Normal Movable" shows this memoryblock
		    can be onlined to ZONE_NORMAL by default and to ZONE_MOVABLE
		    by online_movable.
			
		    "memory7/valid_zones: Movable Normal" shows this memoryblock
		    can be onlined to ZONE_MOVABLE by default and to ZONE_NORMAL
		    by online_kernel.
=================== ============================================================

下面是NT98336平台内存信息
/sys/devices/system/memory
[root@dvrdvs memory] # ls
auto_online_blocks  memory18            memory29
block_size_bytes    memory19            memory3
memory0             memory2             memory30
memory1             memory20            memory31
memory10            memory21            memory4
memory11            memory22            memory5
memory12            memory23            memory6
memory13            memory24            memory7
memory14            memory25            memory8
memory15            memory26            memory9
memory16            memory27            probe
memory17            memory28            uevent
[root@dvrdvs memory] #

[root@dvrdvs memory] # ls memory2
online       phys_index   state        uevent
phys_device  removable    subsystem    valid_zones


[root@dvrdvs memory] # cat memory2/online
1
[root@dvrdvs memory] # cat memory2/phys_device
0
[root@dvrdvs memory] # cat memory2/phys_index
00000002
[root@dvrdvs memory] # cat memory2/removable
1
[root@dvrdvs memory] # cat memory2/state
online
[root@dvrdvs memory] # cat memory2/uevent
[root@dvrdvs memory] # cat memory2/valid_zones
Normal
=================== ============================================================

.. note::
  These directories/files appear after physical memory hotplug phase.

If CONFIG_NUMA is enabled the memoryXXX/ directories can also be accessed 
via symbolic links located in the /sys/devices/system/node/node* directories.

For example: 
/sys/devices/system/node/node0/memory9 -> ../../memory/memory9

A backlink will also be created: 
/sys/devices/system/memory/memory9/node0 -> ../../node/node0

.. _memory_hotplug_physical_mem:
Physical memory hot-add phase
=============================

Hardware(Firmware) Support
--------------------------

On x86_64/ia64 platform, memory hotplug by ACPI is supported.

In general, the firmware (ACPI) which supports memory hotplug defines 
memory class object of _HID "PNP0C80". When a notify is asserted to PNP0C80, 
Linux's ACPI handler does hot-add memory to the system and calls a hotplug 
udev script. This will be done automatically.

But scripts for memory hotplug are not contained in generic udev package(now). 
You may have to write it by yourself or online/offline memory by hand. 
Please see :ref:`memory_hotplug_how_to_online_memory` and 
:ref:`memory_hotplug_how_to_offline_memory`.

If firmware supports NUMA-node hotplug, and defines an object _HID "ACPI0004", 
"PNP0A05", or "PNP0A06", notification is asserted to it, and ACPI handler calls 
hotplug code for all of objects which are defined in it. If memory device is found, 
memory hotplug code will be called.


Notify memory hot-add event by hand
-----------------------------------

On some architectures, the firmware may not notify the kernel of a memory hotplug event.  
Therefore, the memory "probe" interface is supported to explicitly notify the kernel. 
This interface depends on CONFIG_ARCH_MEMORY_PROBE and can be configured on powerpc, 
sh, and x86 if hotplug is supported, although for x86 this should be handled by 
ACPI notification.

Probe interface is located at 
/sys/devices/system/memory/probe

You can tell the physical address of new memory to the kernel by::
	% echo start_address_of_new_memory > /sys/devices/system/memory/probe

Then, [start_address_of_new_memory, start_address_of_new_memory + 
memory_block_size] memory range is hot-added. In this case, hotplug 
script is not called (in current implementation). You'll have to 
online memory by yourself.  Please see :ref:`memory_hotplug_how_to_online_memory`.


Logical Memory hot-add phase 
============================

State of memory
---------------

To see (online/offline) state of a memory block, read 'state' file::
	% cat /sys/device/system/memory/memoryXXX/state

- If the memory block is online, you'll read "online".
- If the memory block is offline, you'll read "offline".

.. _memory_hotplug_how_to_online_memory:

How to online memory
--------------------

When the memory is hot-added, the kernel decides whether or not to "online" 
it according to the policy which can be read from "auto_online_blocks" file::

	% cat /sys/devices/system/memory/auto_online_blocks
	
The default depends on the CONFIG_MEMORY_HOTPLUG_DEFAULT_ONLINE kernel config option. 
If it is disabled the default is "offline" which means the newly added memory is not
in a ready-to-use state and you have to "online" the newly added memory blocks manually.
Automatic onlining can be requested by writing "online" to "auto_online_blocks" file::

	% echo online > /sys/devices/system/memory/auto_online_blocks

This sets a global policy and impacts all memory blocks that will subsequently 
be hotplugged. Currently offline blocks keep their state. It is possible, under 
certain circumstances, that some memory blocks will be added but will fail to online. 
User space tools can check their "state" files 
(/sys/devices/system/memory/memoryXXX/state) and try to online them manually.

If the automatic onlining wasn't requested, failed, or some memory block was 
offlined it is possible to change the individual block's state by writing to the 
"state" file::

	% echo online > /sys/devices/system/memory/memoryXXX/state


This onlining will not change the ZONE type of the target memory block, 
If the memory block doesn't belong to any zone an appropriate kernel zone 
(usually ZONE_NORMAL) will be used unless movable_node kernel command line 
option is specified when ZONE_MOVABLE will be used.

You can explicitly request to associate it with ZONE_MOVABLE by::

	% echo online_movable > /sys/devices/system/memory/memoryXXX/state

.. note:: current limit: this memory block must be adjacent to ZONE_MOVABLE 

Or you can explicitly request a kernel zone (usually ZONE_NORMAL) by::
	
	% echo online_kernel > /sys/devices/system/memory/memoryXXX/state
	
.. note:: current limit: this memory block must be adjacent to ZONE_NORMAL

	
An explicit zone onlining can fail (e.g. when the range is already within 
and existing and incompatible zone already).

After this, memory block XXX's state will be 'online' and the amount of 
available memory will be increased.

This may be changed in future.

Logical memory remove
=====================

Memory offline and ZONE_MOVABLE
-------------------------------

Memory offlining is more complicated than memory online. 
Because memory offline has to make the whole memory block be unused, memory offline can fail if
the memory block includes memory which cannot be freed.

In general, memory offline can use 2 techniques.

(1) reclaim and free all memory in the memory block. 
(2) migrate all pages in the memory block.


In the current implementation, Linux's memory offline uses method (2), freeing 
all  pages in the memory block by page migration. But not all pages are 
migratable. Under current Linux, migratable pages are anonymous pages and 
page caches. For offlining a memory block by migration, the kernel has to 
guarantee that the memory block contains only migratable pages.
在目前的实现中，Linux的内存离线使用方法（2），逐页迁移释放内存中的所有页面。 
但并非所有页面都是可迁移的。在当前的 Linux 下，可迁移页面是匿名页面和页面缓存。 
为了通过迁移使内存块下线，内核必须保证内存块只包含可迁移的页面。


Now, a boot option for making a memory block which consists of migratable pages 
is supported. By specifying "kernelcore=" or "movablecore=" boot option, you can 
create ZONE_MOVABLE...a zone which is just used for movable pages. 
(See also Documentation/admin-guide/kernel-parameters.rst)

Assume the system has "TOTAL" amount of memory at boot time, 
this boot option creates ZONE_MOVABLE as following.

1) When kernelcore=YYYY boot option is used, 
   Size of memory not for movable pages (not for offline) is YYYY.
   Size of memory for movable pages (for offline) is TOTAL-YYYY.

2) When movablecore=ZZZZ boot option is used, 
	Size of memory not for movable pages (not for offline) is TOTAL - ZZZZ. 
	Size of memory for movable pages (for offline) is ZZZZ.

.. note::
   Unfortunately, there is no information to show which memory block belongs 
   to ZONE_MOVABLE. This is TBD.
   
.. _memory_hotplug_how_to_offline_memory:

============================================================================
[root@dvrdvs memory] # ls
auto_online_blocks  memory18            memory29
block_size_bytes    memory19            memory3
memory0             memory2             memory30
memory1             memory20            memory31
memory10            memory21            memory4
memory11            memory22            memory5
memory12            memory23            memory6
memory13            memory24            memory7
memory14            memory25            memory8
memory15            memory26            memory9
memory16            memory27            probe
memory17            memory28            uevent

[root@dvrdvs memory] # hik_echo 0x80000000 > /sys/devices/system/memory/probe
[root@dvrdvs memory] # ls
auto_online_blocks  memory19            memory30
block_size_bytes    memory2             memory31
memory0             memory20            memory32
memory1             memory21            memory4
memory10            memory22            memory5
memory11            memory23            memory6
memory12            memory24            memory7
memory13            memory25            memory8
memory14            memory26            memory9
memory15            memory27            probe
memory16            memory28            uevent
memory17            memory29
memory18            memory3
[root@dvrdvs memory] # free
             total       used       free     shared    buffers     cached
Mem:       2041436     167180    1874256      20184        688      29288
-/+ buffers/cache:     137204    1904232
Swap:            0          0          0
[root@dvrdvs memory] #ls
auto_online_blocks  memory19            memory30
block_size_bytes    memory2             memory31
memory0             memory20            memory32
memory1             memory21            memory4
memory10            memory22            memory5
memory11            memory23            memory6
memory12            memory24            memory7
memory13            memory25            memory8
memory14            memory26            memory9
memory15            memory27            probe
memory16            memory28            uevent
memory17            memory29
memory18            memory3
[root@dvrdvs memory] # cd memory32/
[root@dvrdvs memory32] # ls
online       phys_index   state        uevent
phys_device  removable    subsystem    valid_zones
[root@dvrdvs memory32] # cat online
0
[root@dvrdvs memory32] # cat ../memory31/online
1
[root@dvrdvs memory32] # cat state
offline
[root@dvrdvs memory32] # hik_echo 1 > online
[  184.014387] min_free_kbytes is not updated to 5664 because user defined value 67584 is preferred
[root@dvrdvs memory32] #
[root@dvrdvs memory32] # free
             total       used       free     shared    buffers     cached
Mem:       2106972     167464    1939508      20184        688      29288
-/+ buffers/cache:     137488    1969484
Swap:            0          0          0
[root@dvrdvs memory32] #
[root@dvrdvs memory32] # cat valid_zones
Normal

============================================================================
How to offline memory
---------------------

You can offline a memory block by using the same sysfs interface that was used 
in memory onlining::

	% echo offline > /sys/devices/system/memory/memoryXXX/state

If offline succeeds, the state of the memory block is changed to be "offline". 
If it fails, some error core (like -EBUSY) will be returned by the kernel. 
Even if a memory block does not belong to ZONE_MOVABLE, you can try to offline it.  
If it doesn't contain 'unmovable' memory, you'll get success.

失败的时候 多试几次就OK的。

hik_echo offline > /sys/devices/system/memory/memory32/state

[root@dvrdvs memory] # cat memory32/online
0
[root@dvrdvs memory] # cat memory32/state
offline
[root@dvrdvs memory] # cat memory32/valid_zones
Normal Movable										---变回了最初的Movable属性
[root@dvrdvs memory] #

A memory block under ZONE_MOVABLE is considered to be able to be offlined 
easily.  But under some busy state, it may return -EBUSY. Even if a memory 
block cannot be offlined due to -EBUSY, you can retry offlining it and may be 
able to offline it (or not). (For example, a page is referred to by some kernel 
internal call and released soon.)

Consideration: 
	Memory hotplug's design direction is to make the possibility of memory 
	offlining higher and to guarantee unplugging memory under any situation. 
	But it needs more work. Returning -EBUSY under some situation may be good because 
	the user can decide to retry more or not by himself. Currently, memory offlining 
	code does some amount of retry with 120 seconds timeout.
考虑：
内存热插拔的设计方向是让内存下线的可能性更高，保证在任何情况下都可以拔掉内存。 
但它需要更多的工作。 在某些情况下返回 -EBUSY 可能会很好，因为用户可以自己决定是否重试。 
目前，内存脱机代码会在 120 秒超时后进行一些重试。

Physical memory remove 
======================

Need more implementation yet....
 - Notification completion of remove works by OS to firmware.
 - Guard from remove if not yet.
 
Memory hotplug event notifier 
============================= 

Hotplugging events are sent to a notification queue.

There are six types of notification defined in include/linux/memory.h:

MEM_GOING_ONLINE 
Generated before new memory becomes available in order to be able to 
prepare subsystems to handle memory. The page allocator is still unable 
to allocate from the new memory.

MEM_CANCEL_ONLINE 
Generated if MEMORY_GOING_ONLINE fails.

MEM_ONLINE 
Generated when memory has successfully brought online. The callback may 
allocate pages from the new memory.

MEM_GOING_OFFLINE 
Generated to begin the process of offlining memory. Allocations are no 
longer possible from the memory but some of the memory to be offlined
is still in use. The callback can be used to free memory known to a subsystem 
from the indicated memory block.

MEM_CANCEL_OFFLINE 
Generated if MEMORY_GOING_OFFLINE fails. Memory is available again from the memory 
block that we attempted to offline.

MEM_OFFLINE 
Generated after offlining memory is complete.

A callback routine can be registered by calling::
  hotplug_memory_notifier(callback_func, priority)
  
Callback functions with higher values of priority are called before callback functions with lower values.
A callback function must have the following prototype::

  int callback_func( struct notifier_block *self, unsigned long action, void *arg);
  
The first argument of the callback function (self) is a pointer to the block 
of the notifier chain that points to the callback function itself. 
The second argument (action) is one of the event types described above. 
The third argument (arg) passes a pointer of struct memory_notify::

	struct memory_notify { unsigned long start_pfn;
		unsigned long nr_pages;
		int status_change_nid_normal;
		int status_change_nid_high;
		int status_change_nid; }
		
- start_pfn is start_pfn of online/offline memory.
- nr_pages is # of pages of online/offline memory.
- status_change_nid_normal is set node id when N_NORMAL_MEMORY of nodemask 
is (will be) set/clear, if this is -1, then nodemask status is not changed.
- status_change_nid_high is set node id when N_HIGH_MEMORY of nodemask is (will be) 
set/clear, if this is -1, then nodemask status is not changed.
- status_change_nid is set node id when N_MEMORY of nodemask is (will be) set/clear. 
It means a new(memoryless) node gets new memory by online and a node loses all memory. 
If this is -1, then nodemask status is not changed.
 
If status_changed_nid* >= 0, callback should create/discard structures for the 
node if necessary.

The callback routine shall return one of the values 
NOTIFY_DONE, NOTIFY_OK, NOTIFY_BAD, NOTIFY_STOP
 defined in include/linux/notifier.h
 
NOTIFY_DONE and NOTIFY_OK have no effect on the further processing.

NOTIFY_BAD is used as response to the MEM_GOING_ONLINE, MEM_GOING_OFFLINE, 
MEM_ONLINE, or MEM_OFFLINE action to cancel hotplugging. It stops further
processing of the notification queue.
 
NOTIFY_STOP stops further processing of the notification queue.



代码实现流程
====> drivers/base/memory.c
总的入口
memory_dev_init
在哪里调用进来的呢？
start_kernel
--->rest_init
---|--->kernel_init
---|---|--->kernel_init_freeable
---|---|---|--->do_basic_setup
---|---|---|---|--->driver_init
---|---|---|---|---|--->memory_dev_init   ：没有定义HOTPLUG_SPARSE是直接返回0

它在干什么呢？
1、在/sys/device/system下注册memory节点
	subsys_system_register(&memory_subsys, memory_root_attr_groups);
2、获取块大小
	get_memory_block_size();
3、计算每一个sections_per_block的大小，这里section=block
	sections_per_block = block_sz / MIN_MEMORY_BLOCK_SIZE;
4、从0开始直到最大值__highest_present_section_nr依次创建memoryXXX
	add_memory_block(i);

int __init memory_dev_init(void)
{
	unsigned int i;
	int ret;
	int err;
	unsigned long block_sz;

    pr_notice("GYK memory_dev_init\n");

	ret = subsys_system_register(&memory_subsys, memory_root_attr_groups);
	if (ret)
		goto out;

	block_sz = get_memory_block_size();
	sections_per_block = block_sz / MIN_MEMORY_BLOCK_SIZE;

	/*
	 * Create entries for memory sections that were found
	 * during boot and have been initialized
	 */
	mutex_lock(&mem_sysfs_mutex);
	for (i = 0; i <= __highest_present_section_nr;
		i += sections_per_block) {
		err = add_memory_block(i);
		if (!ret)
			ret = err;
	}
	mutex_unlock(&mem_sysfs_mutex);

out:
	if (ret)
		printk(KERN_ERR "%s() failed: %d\n", __func__, ret);
	return ret;
}

__highest_present_section_nr来自哪里呢？
mm/sparse.c中接口section_mark_present

int __highest_present_section_nr;
static void section_mark_present(struct mem_section *ms)
{
	int section_nr = __section_nr(ms);

	if (section_nr > __highest_present_section_nr)
		__highest_present_section_nr = section_nr;

	ms->section_mem_map |= SECTION_MARKED_PRESENT;
}

什么阶段在执行section存在的标记动作？

void __init memory_present(int nid, unsigned long start, unsigned long end)
{
	unsigned long pfn;

#ifdef CONFIG_SPARSEMEM_EXTREME
	if (unlikely(!mem_section)) {
		unsigned long size, align;

		size = sizeof(struct mem_section*) * NR_SECTION_ROOTS;
		align = 1 << (INTERNODE_CACHE_SHIFT);
		mem_section = memblock_virt_alloc(size, align);
	}
#endif

	start &= PAGE_SECTION_MASK;
	mminit_validate_memmodel_limits(&start, &end);
	for (pfn = start; pfn < end; pfn += PAGES_PER_SECTION) {
		unsigned long section = pfn_to_section_nr(pfn);
		struct mem_section *ms;

		sparse_index_init(section, nid);
		set_section_nid(section, nid);

		ms = __nr_to_section(section);
		if (!ms->section_mem_map) {
			ms->section_mem_map = sparse_encode_early_nid(nid) |
							SECTION_IS_ONLINE;
			section_mark_present(ms);
		}
	}
}


借此机会先来看下，memblock的初始化
了解一个模块，要先了解其数据结构

/**
 * enum memblock_flags - definition of memory region attributes
 * @MEMBLOCK_NONE: no special request
 * @MEMBLOCK_HOTPLUG: hotpluggable region
 * @MEMBLOCK_MIRROR: mirrored region
 * @MEMBLOCK_NOMAP: don't add to kernel direct mapping
 */
enum memblock_flags {
	MEMBLOCK_NONE		= 0x0,	/* No special request */
	MEMBLOCK_HOTPLUG	= 0x1,	/* hotpluggable region */
	MEMBLOCK_MIRROR		= 0x2,	/* mirrored region */
	MEMBLOCK_NOMAP		= 0x4,	/* don't add to kernel direct mapping */
};

/**
 * struct memblock_region - represents a memory region
 * @base: physical address of the region
 * @size: size of the region
 * @flags: memory region attributes
 * @nid: NUMA node id
 */
struct memblock_region {
	phys_addr_t base;
	phys_addr_t size;
	enum memblock_flags flags;
#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP
	int nid;
#endif
};

/**
 * struct memblock_type - collection of memory regions of certain type
 * @cnt: number of regions
 * @max: size of the allocated array
 * @total_size: size of all regions
 * @regions: array of regions
 * @name: the memory type symbolic name
 */
struct memblock_type {
	unsigned long cnt;
	unsigned long max;
	phys_addr_t total_size;
	struct memblock_region *regions;
	char *name;
};

/**
 * struct memblock - memblock allocator metadata
 * @bottom_up: is bottom up direction?
 * @current_limit: physical address of the current allocation limit
 * @memory: usabe memory regions
 * @reserved: reserved memory regions
 * @physmem: all physical memory
 */
struct memblock {
	bool bottom_up;  /* is bottom up direction? */
	phys_addr_t current_limit;
	struct memblock_type memory;
	struct memblock_type reserved;
#ifdef CONFIG_HAVE_MEMBLOCK_PHYS_MAP
	struct memblock_type physmem;
#endif
};

另外有一个大的静态数据
struct memblock memblock __initdata_memblock = {
	.memory.regions		= memblock_memory_init_regions,
	.memory.cnt			= 1,			/* empty dummy entry */
	.memory.max			= INIT_MEMBLOCK_REGIONS,
	.memory.name		= "memory",

	.reserved.regions	= memblock_reserved_init_regions,
	.reserved.cnt		= 1,				/* empty dummy entry */
	.reserved.max		= INIT_MEMBLOCK_REGIONS,
	.reserved.name		= "reserved",

#ifdef CONFIG_HAVE_MEMBLOCK_PHYS_MAP
	.physmem.regions	= memblock_physmem_init_regions,
	.physmem.cnt		= 1,			/* empty dummy entry */
	.physmem.max		= INIT_PHYSMEM_REGIONS,
	.physmem.name		= "physmem",
#endif

	.bottom_up		= false,
	.current_limit		= MEMBLOCK_ALLOC_ANYWHERE,
};

arch/arm64/mm/init.c
bootmem_init


memblock怎么组建起来的？

设备树资源：
memory {
		device_type = "memory";
		reg = <0x0 0x0 0x2 0x0>;
};

reserved-memory {
		#address-cells = <0x2>;
		#size-cells = <0x2>;
		ranges;

		atf@0x01F00000 {
				reg = <0x0 0x1f00000 0x0 0x100000>;
				no-map;
		};

		cma0@0x05000000 {
				compatible = "shared-dma-pool";
				reusable;
				reg = <0x0 0x5000000 0x0 0x800000>;
				alignment = <0x0 0x400000>;
				status = "okay";
				phandle = <0x7>;
		};

		logo@0x10000000 {
				reg = <0x0 0x10000000 0x0 0x800000>;
				no-map;
		};
};


start_kernel
--->setup_arch
---|--->setup_machine_fdt(__fdt_pointer); //__fdt_pointer 是head.S中来自x21，NT上是0x1cff8000
---|---|--->fixmap_remap_fdt
---|---|---|--->memblock_reserve(dt_phys, size);
---|---|---|---|--->memblock_add_range(&memblock.reserved, base, size, MAX_NUMNODES, 0)
---|---|---|---|---|--->每种类型第一次进入时执行
						type->regions[0].base = base;
						type->regions[0].size = size;
						type->regions[0].flags = flags;
---|---|---|---|---|--->memblock_set_region_node(&type->regions[0], nid); //空函数
---|---|---|---|---|--->type->total_size = size;
---|---|--->early_init_dt_scan
---|---|---|--->early_init_dt_scan_memory
---|---|---|---|--->early_init_dt_add_memory_arch(base, size); //base=0,size=0x200000000
---|---|---|---|---|--->memblock_add(base, size);
---|---|---|---|---|---|--->memblock_add_range(&memblock.memory, base, size, MAX_NUMNODES, 0);//将8GB的内存添加到了memory中
---|--->arm64_memblock_init();
---|---|--->memblock_remove(1ULL << PHYS_MASK_SHIFT, ULLONG_MAX);
---|---|---|--->memblock_remove_range(&memblock.memory, base, size);
---|---|---|--->memblock_remove(max_t(u64, memstart_addr + linear_region_size, __pa_symbol(_end)), ULLONG_MAX);
---|---|---|--->memblock_mem_limit_remove_map(memory_limit);
---|---|---|--->memblock_add(__pa_symbol(_text), (u64)(_end - _text)); //将kernel code段增加进来
---|---|---|--->依据设备树中reserved-memory信息，将整个内存分成多个section
---|---|---|--->将ramdisk用到的内存remove先掉，再add进来，再reserve
---|---|---|--->将kernel code段进行reserve
---|---|---|--->将设备树内存地址进行reserve
---操作接口很简单：memblock_add_range(&memblock.reserved, base, size, MAX_NUMNODES, 0);

疑问：
linear_region_size 仅是在bootmem阶段对kernel image使用的吗？

内核中真的还存在线性映射吗？直接以ioremap映射查看其物理地址与虚拟地址的关系？


将起始地址为base，长度为size的物理内存添加到reserve中，意味着已使用保存起来，后续伙伴系统不可见
int __init_memblock memblock_reserve(phys_addr_t base, phys_addr_t size)
{
	phys_addr_t end = base + size - 1;

	memblock_dbg("memblock_reserve: [%pa-%pa] %pF\n",
		     &base, &end, (void *)_RET_IP_);

	return memblock_add_range(&memblock.reserved, base, size, MAX_NUMNODES, 0);
}

















